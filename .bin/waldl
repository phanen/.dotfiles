#!/usr/bin/env bash

# thumbnails -> open with sxiv -> select -> quit -> download

walldir="$HOME/pic"
cachedir="$HOME/.cache/pic" # dir to cache thumbnails
sxiv_opts=" -tfpo -z 200" # o is needed for selection
max_pages=4 # 24 results per page
sorting=favorites # date_added, relevance, random, views, favorites, toplist
quality=large # large/original/small
atleast=1920x1080 # least/res

# [ -e "$HOME/.config/waldlrc" ] && . "$HOME/.config/waldlrc"

# use dmenu when without args
sh_menu () {
	# : | dmenu -p "search wallhaven:"
	rofi -dmenu -l 0 -p "search wallpapers"
}

# progress display
sh_info () {
	printf "%s\n" "$1" >&2
	notify-send "wallhaven" "$1"
	# err handler
	[ -n "$2" ] && exit "$2"
}

dep_ck () {
	for prog; do # syntax sugar?
		command -v "$prog" >/dev/null 2>&1 || 
			sh_info "command $prog not found, install: $prog"
	done
}

clean_up () {
	printf "%s\n" "cleaning up..." >&2
	rm -rf "$jsonfile" "$cachedir"
}

# get json for each page, from query
get_json () {
	for page_no in $(seq $max_pages)
	do
		{
			json=$(curl -s -G "https://wallhaven.cc/api/v1/search" \
				-d "q=$1" \
				-d "page=$page_no" \
				-d "atleast=$atleast" \
				-d "sorting=$sorting"
			)
			printf "%s\n" "$json" >> "$jsonfile"
		} &
		sleep 0.001
	done
	wait
}

############################################################################

# ensure dest dir and deps
mkdir -p "$walldir" "$cachedir"
dep_ck "sxiv" "curl" "jq"

# file to store the json
jsonfile="/tmp/wald.$$"

# get query then parse
[ -n "$*" ] && query="$*" || query=$(sh_menu)
[ -z "$query" ] && exit 1
query=$(printf '%s' "$query" | tr ' ' '+' )

# refresh old cache
rm -rf "$cachedir" && mkdir -p "$cachedir"

# clean up if killed
trap "exit" INT TERM
trap "clean_up" EXIT

# query -> json -> thumbnails
sh_info "getting data..."
get_json "$query"
# if get nothing, then exit
[ -s "$jsonfile" ] || sh_info "no images found" 1 
# else get thumnails from json
thumbnails=$(jq -r '.data[]?|.thumbs.'"$quality" < "$jsonfile")
[ -z "$thumbnails" ] && sh_info "no-results found" 1

# download the thumbnails
sh_info "caching thumbnails..."
for url in $thumbnails
do
	printf "url = %s\n" "$url"
	printf "output = %s\n" "$cachedir/${url##*/}"
done | curl -Z -K -
sh_info "downloaded thumbnails..."

# sxiv as selector
image_ids="$(sxiv $sxiv_opts "$cachedir")"
echo $image_ids
[ -z "$image_ids" ] && exit

# download the selected wallpapers
cd "$walldir" || exit
sh_info "downloading wallpapers..."
for ids in $image_ids
do
	ids="${ids##*/}"
	ids="${ids%.*}"
	url=$( jq -r '.data[]?|select( .id == "'$ids'" )|.path' < "$jsonfile" )
	printf "url = %s\n" "$url"
	printf -- "-O\n"
done | curl -K -

sh_info "wallpapers downloaded in:- '$walldir'"
sxiv "$(ls -c)" # sort by ctime
